{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis of Bayesian optimization process\n",
    "*Note*: In the following I made some changes in the code that I used for training the main model. For example, I changed the default params, or I reduced the total number of evaluations in the bayesian optimization in order to save time for finding optimal hyperparameters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9033eb3d41210763"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff75c5bc4f67e292"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skopt.space import Integer\n",
    "from skopt.space import Real\n",
    "from skopt.space import Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf349d9743e1fab4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def dir_id(directory: str) -> int:\n",
    "    return {'anger': 0,\n",
    "            'contempt': 1,\n",
    "            'disgust': 2,\n",
    "            'fear': 3,\n",
    "            'happy': 4,\n",
    "            'sadness': 5,\n",
    "            'surprise': 6\n",
    "            }[directory]\n",
    "\n",
    "\n",
    "def preprocess_image(img_path: str, width: int, height: int) -> np.ndarray:\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (width, height))\n",
    "    img = np.array(img)\n",
    "    img = img.reshape((1,) + img.shape)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_data(data: str, width: int, height: int) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Assuming all pictures are structured in subdirectories names as the corresponding expression\n",
    "    :param data: str containing the root directory of the data\n",
    "    :param width: goal width of image\n",
    "    :param height: goal height of image\n",
    "    :return: tuple (evidence, labels)\n",
    "    \"\"\"\n",
    "    evidence = []\n",
    "    labels = []\n",
    "\n",
    "    subdirectories = [directory for directory in os.listdir(data) if directory[0] != '.']\n",
    "\n",
    "    for subdirectory in subdirectories:\n",
    "        if os.path.isdir(os.path.join(data, subdirectory)):\n",
    "            for img_path in os.listdir(os.path.join(data, subdirectory)):\n",
    "                img = preprocess_image(os.path.join(data, subdirectory, img_path), width, height)\n",
    "                evidence.append(img)\n",
    "                labels.append(dir_id(subdirectory))\n",
    "\n",
    "    return evidence, labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53cd506e55ed9f5d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create CNN based on hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a581f47cbf0c5636"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_model(width, height, categories,\n",
    "                 num_convolutions, num_convolution_filters, pool_size, learning_rate,\n",
    "                 num_dense_layers, num_dense_nodes, activation, dropout):\n",
    "    \"\"\"\n",
    "    Returns a compiled CNN model\n",
    "    hyperparameters:\n",
    "    num_convolutions:           number of convolutional layers\n",
    "    num_convolution_filters:    number of convolutional filters per convolutional layer\n",
    "    pool_size:                  pool size for pooling layers\n",
    "    learning_rate:              Learning-rate for the optimizer\n",
    "    number_dense_layers:        Number of dense layers\n",
    "    number_dense_nodes:         Number of nodes in each dense layer\n",
    "    activation:                 Activation function for all layers\n",
    "    dropout:                    Dropout\n",
    "    \"\"\"\n",
    "\n",
    "    # Start construction of a Keras Sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Input(shape=(width, height, 1)))\n",
    "\n",
    "    # convolutional layers and pooling with the respective sizes\n",
    "    for i in range(num_convolutions):\n",
    "        model.add(tf.keras.layers.Conv2D(kernel_size=5, strides=1, filters=num_convolution_filters, padding='same',\n",
    "                                              activation=activation, name=f'layer_conv{i}'))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(pool_size=(pool_size, pool_size), strides=2))\n",
    "\n",
    "    # flatten out the data\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # fully-connected / dense layers.\n",
    "    for i in range(num_dense_layers):\n",
    "        model.add(tf.keras.layers.Dense(num_dense_nodes, activation=activation))\n",
    "\n",
    "    # Add dropout to prevent over fitting\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout))\n",
    "\n",
    "    # Last fully-connected / dense layer with softmax-activation\n",
    "    # for use in classification\n",
    "    model.add(tf.keras.layers.Dense(units=categories, activation='softmax'))\n",
    "\n",
    "    # Use the Adam method for training the network\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy']\n",
    "                  )\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "867d5fca43920975"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bayesian optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "464d9214db2a22b8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define fix parameters\n",
    "DATA = 'CK+48'\n",
    "TEST_SIZE = 0.3\n",
    "IMG_WIDTH = 48\n",
    "IMG_HEIGHT = 48\n",
    "EPOCHS = 10\n",
    "CATEGORIES = 7\n",
    "BEST_MODEL_PATH = 'current_best_model.keras'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ae21e1e09266c68"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the search space that our bayesian optimization algorithm shall search on"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7077c2440b61f1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define search space\n",
    "dim_num_convolutions = Integer(low=0, high=4, name='num_convolutions')\n",
    "dim_num_convolution_filters = Integer(low=4, high=64, name='num_convolution_filters')\n",
    "dim_pool_size = Integer(low=2, high=6, name='pool_size')\n",
    "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform', name='learning_rate')\n",
    "dim_number_dense_layers = Integer(low=1, high=5, name='number_dense_layers')\n",
    "dim_number_dense_nodes = Integer(low=5, high=1024, name='number_dense_nodes')\n",
    "dim_activation = Categorical(categories=['relu', 'sigmoid'], name='activation')\n",
    "dim_dropout = Real(low=0, high=0.99, prior='uniform', name='dropout')\n",
    "\n",
    "dimensions = [\n",
    "    dim_num_convolutions,\n",
    "    dim_num_convolution_filters,\n",
    "    dim_pool_size,\n",
    "    dim_learning_rate,\n",
    "    dim_number_dense_layers,\n",
    "    dim_number_dense_nodes,\n",
    "    dim_activation,\n",
    "    dim_dropout\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13a42ab242c98d7e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define some default parameters, this is the starting point of the bayesian optimization.\n",
    "Best accuracy will keep track of our current best accuracy and thus the best model we want to store.\n",
    "Then we load the data and split it into train and test set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8d4b9e5b3a1e257"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "default_params = [1, 32, 2, 1e-5, 1, 512, 'relu', 0.5]\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# load data\n",
    "evidence, labels = load_data(DATA, IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(evidence), np.array(labels), test_size=TEST_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "214ea0e14cd4e36c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define an evaluate function. It creates the model, augments the data, trains and evaluates the model on the test set and then returns the weighted accuracy of test and train accuracy to prevent overfitting on one of the test sets."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c081321bbd4baa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@use_named_args(dimensions)\n",
    "def evaluate(num_convolutions, num_convolution_filters, pool_size, learning_rate, number_dense_layers,\n",
    "             number_dense_nodes, activation, dropout) -> float:\n",
    "    \"\"\"\n",
    "    hyperparameters:\n",
    "    num_convolutions:           number of convolutional layers\n",
    "    num_convolution_filters:    number of convolutional filters per convolutional layer\n",
    "    pool_size:                  pool size for pooling layers\n",
    "    learning_rate:              Learning-rate for the optimizer\n",
    "    number_dense_layers:        Number of dense layers\n",
    "    number_dense_nodes:         Number of nodes in each dense layer\n",
    "    activation:                 Activation function for all layers\n",
    "    dropout:                    Dropout\n",
    "    \"\"\"\n",
    "\n",
    "    # Print the hyperparameters.\n",
    "    print('num_convolutions: ', num_convolutions)\n",
    "    print('num_convolution_filters: ', num_convolution_filters)\n",
    "    print('pool_size: ', pool_size)\n",
    "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
    "    print('num_dense_layers:', number_dense_layers)\n",
    "    print('num_dense_nodes:', number_dense_nodes)\n",
    "    print('activation:', activation)\n",
    "    print('dropout:', dropout)\n",
    "    print()\n",
    "\n",
    "    # case that pooling would reduce the image to negative dimensions producing an error\n",
    "    if min(IMG_WIDTH, IMG_HEIGHT) / (pool_size ** num_convolutions) < 1:\n",
    "        return 1\n",
    "\n",
    "    # Create the neural network with these hyperparameters.\n",
    "    model = create_model(width=IMG_WIDTH,\n",
    "                         height=IMG_HEIGHT,\n",
    "                         categories=CATEGORIES,\n",
    "                         num_convolutions=num_convolutions,\n",
    "                         num_convolution_filters=num_convolution_filters,\n",
    "                         pool_size=pool_size,\n",
    "                         learning_rate=learning_rate,\n",
    "                         num_dense_layers=number_dense_layers,\n",
    "                         num_dense_nodes=number_dense_nodes,\n",
    "                         activation=activation,\n",
    "                         dropout=dropout)\n",
    "\n",
    "    # data augmentation\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "    global x_train\n",
    "    global x_test\n",
    "\n",
    "    # Reshape images for compatibility with the augmentation\n",
    "    x_train = x_train.reshape((-1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "    x_test = x_test.reshape((-1, IMG_WIDTH, IMG_HEIGHT, 1))\n",
    "\n",
    "    train_generator = train_datagen.flow(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=32,\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        batch_size=32,\n",
    "    )\n",
    "\n",
    "    # train model and extract accuracy\n",
    "    trained = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=test_generator\n",
    "    )\n",
    "\n",
    "    trained_accuracy = trained.history['val_accuracy'][-1]\n",
    "    tested_accuracy = model.evaluate(test_generator)[1]\n",
    "\n",
    "    # calculated weighted average of tested and trained accuracy to prevent overfitting of the bayesian\n",
    "    # optimization on one of the test sets\n",
    "    accuracy = (3*tested_accuracy + trained_accuracy) / 4\n",
    "\n",
    "    # print the classification accuracy\n",
    "    print()\n",
    "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
    "    print()\n",
    "\n",
    "    # update the best accuracy\n",
    "    global best_accuracy\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        model.save(BEST_MODEL_PATH)\n",
    "\n",
    "    del model\n",
    "\n",
    "    return 1 - accuracy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "588644d6858139b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now all that is left to do is to use scikit's gp_minimize function, which uses gaussian processes for minimizing the evaluate function, thus maximizing accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d5bb261b6215f1a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def optimize():\n",
    "    \"\"\"\n",
    "    Performs bayesian optimization\n",
    "    \"\"\"\n",
    "    result = gp_minimize(func=evaluate,\n",
    "                         dimensions=dimensions,\n",
    "                         acq_func='EI',\n",
    "                         n_calls=40,\n",
    "                         x0=default_params)\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fda4476f09f97917"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "res = optimize()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c645c0816f02450"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb73aebdae996c43"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# optimal hyperparameters found\n",
    "print(f\"{res.x} lead to an accuracy of {res.fun}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee75bc9654b03b1b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence, plot_objective, plot_evaluations\n",
    "\n",
    "# *note*: the following plots may take a while to render\n",
    "\n",
    "# plots how the minimum of the objective function converged to the end result\n",
    "plot_convergence(res)\n",
    "\n",
    "# shows when in the search process which (combinations of) dimensions where evaluated\n",
    "plot_evaluations(res)\n",
    "\n",
    "# shows partial dependencies of dimensions, i.e. the influence a certain hyperparameter has on the objective function\n",
    "plot_objective(res)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6469dc925282e666"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
